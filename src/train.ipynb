{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import torch.utils.data\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from time import time\n",
    "import pandas as pd\n",
    "from classes.policy_network import PolicyNetwork\n",
    "from classes.diffusion_model import DiffusionModel\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'mps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_batch(size):\n",
    "    x, _ = make_swiss_roll(size)\n",
    "    x = x[:, [2, 0]] / 10.0 * np.array([1, -1])\n",
    "    return x[:, 0].reshape((1, size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(features, weights, bias=None):\n",
    "    # Calculate the linear combination\n",
    "    logits = torch.matmul(features, weights)\n",
    "\n",
    "    # If bias is provided, add it to the logits\n",
    "    if bias is not None:\n",
    "        logits += bias\n",
    "\n",
    "    # Apply the sigmoid function to get the probabilities\n",
    "    probabilities = torch.sigmoid(logits)\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"08\"\n",
    "path_to_weights = (\n",
    "    \"/Users/hazimiasad/Documents/Work/megan/data/collection/Study1/sub-\"\n",
    "    + model_name\n",
    "    + \"/pattern/dc_weights.csv\"\n",
    ")\n",
    "weights = torch.from_numpy(pd.read_csv(path_to_weights, header=None).values.T).to(\n",
    "    DEVICE, dtype=torch.float32\n",
    ")\n",
    "state_size = len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(epoch, model, optimizer, loss, rewards, name, batch_size=32, max_grad_norm=1):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'batch_size':batch_size,\n",
    "        'max_grad_norm':max_grad_norm,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "        'rewards': rewards\n",
    "    }, \"/Users/hazimiasad/Documents/Work/megan/code/playground/RL-Diffusion/results/models/sub-\"+model_name+\"/\"+name+\".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rl(model, optimizer, reward_function, state_size, nb_epochs=150_000, batch_size=32, device='cpu', max_norm=.5):\n",
    "    training_loss = []\n",
    "    rewards = []\n",
    "    for epoch in tqdm(range(nb_epochs), mininterval=60.0):\n",
    "        batch_loss = 0\n",
    "        batch_reward = 0\n",
    "        for b in range(batch_size):\n",
    "            x0 = torch.from_numpy(sample_batch(state_size)).float().to(device)\n",
    "            t = 40\n",
    "            mu_posterior, sigma_posterior, x = model.forward_process(x0, t)\n",
    "            log_probs = []\n",
    "            for t in range(40, 0, -1):\n",
    "                # mu, sigma, x = model.reverse(x, t)\n",
    "                action, log_prob, _, _ = model.select_action(x, t)\n",
    "                log_probs.append(log_prob)\n",
    "                x = x + action\n",
    "        \n",
    "\n",
    "            reward = reward_function(x, weights)\n",
    "            batch_reward += reward\n",
    "            reward = torch.tensor(reward, dtype=torch.float32).to(device)\n",
    "            if torch.isnan(reward).any():\n",
    "                print(f\"NaN detected in reward at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "            log_probs_stack_sum = torch.stack(log_probs).sum()\n",
    "            loss = -reward * log_probs_stack_sum\n",
    "            batch_loss += loss\n",
    "        loss = batch_loss/batch_size\n",
    "        reward = batch_reward/batch_size\n",
    "        if torch.isnan(loss).any():\n",
    "            print(f\"NaN detected in loss at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Check gradients\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None and torch.isnan(param.grad).any():\n",
    "                print(f\"NaN detected in gradients for {name} at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "        optimizer.step()\n",
    "        training_loss.append(loss.item())\n",
    "        rewards.append(reward)\n",
    "        name = str(time.time()) +'_epoch_'+str(epoch)\n",
    "        save_model(epoch, model, optimizer, training_loss, rewards, name)\n",
    "        # Check parameters\n",
    "        for name, param in model.named_parameters():\n",
    "            if torch.isnan(param).any():\n",
    "                print(f\"NaN detected in parameters for {name} at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "    return training_loss, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reward(model, optimizer, reward_function, state_size, nb_epochs=150_000, batch_size=32, device='cpu', max_norm=0.5):\n",
    "    \"\"\"\n",
    "    Trains the diffusion model to maximize a reward signal computed on the final output.\n",
    "    This version does not use RL (e.g. REINFORCE) but instead backpropagates directly through\n",
    "    a deterministic reverse diffusion process.\n",
    "\n",
    "    Args:\n",
    "        model: the diffusion model (which must implement forward_process() and reverse())\n",
    "        optimizer: an optimizer for the model parameters\n",
    "        reward_function: a callable that takes (x, weights) and returns a reward (assumed differentiable)\n",
    "        state_size: size/shape information for sample_batch\n",
    "        nb_epochs: number of training epochs\n",
    "        batch_size: batch size per epoch\n",
    "        device: device to use ('cpu' or 'cuda')\n",
    "        max_norm: maximum gradient norm for clipping\n",
    "\n",
    "    Returns:\n",
    "        training_loss: list of loss values per epoch\n",
    "        rewards: list of average rewards per epoch\n",
    "    \"\"\"\n",
    "    training_loss = []\n",
    "    rewards = []\n",
    "\n",
    "    for epoch in tqdm(range(nb_epochs), mininterval=60.0):\n",
    "        batch_loss = 0.0\n",
    "        batch_reward = 0.0\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            # Sample an initial state x0\n",
    "            x0 = torch.from_numpy(sample_batch(state_size)).float().to(device)\n",
    "            t = 40\n",
    "\n",
    "            # Run the forward process (adding noise) to obtain a noisy sample x\n",
    "            mu_posterior, sigma_posterior, x = model.forward_process(x0, t)\n",
    "\n",
    "            # Instead of sampling actions via a policy (as in RL), run a deterministic reverse process\n",
    "            # For each diffusion step, use the model's reverse function to gradually denoise x.\n",
    "            for step in range(40, 0, -1):\n",
    "                # Here we assume model.reverse(x, t) returns the denoised x at step t.\n",
    "                # (If your model requires stochasticity you may need to adjust this accordingly.)\n",
    "                x = model.reverse(x, step)\n",
    "\n",
    "            # Compute the reward signal from the final sample x.\n",
    "            # (Make sure that the variable `weights` is defined or passed as an argument as needed.)\n",
    "            reward_val = reward_function(x, weights)\n",
    "            # Convert reward to a tensor on the correct device.\n",
    "            reward_tensor = torch.tensor(reward_val, dtype=torch.float32, device=device)\n",
    "            if torch.isnan(reward_tensor).any():\n",
    "                print(f\"NaN detected in reward at epoch {epoch}\")\n",
    "                continue\n",
    "\n",
    "            # We want to maximize the reward, so we minimize its negative.\n",
    "            loss = -reward_tensor\n",
    "            batch_loss += loss\n",
    "            batch_reward += reward_tensor.item()\n",
    "\n",
    "        # Average the losses and rewards over the batch.\n",
    "        loss = batch_loss / batch_size\n",
    "        avg_reward = batch_reward / batch_size\n",
    "\n",
    "        if torch.isnan(loss).any():\n",
    "            print(f\"NaN detected in loss at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Check gradients for NaNs.\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None and torch.isnan(param.grad).any():\n",
    "                print(f\"NaN detected in gradients for {name} at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        training_loss.append(loss.item())\n",
    "        rewards.append(avg_reward)\n",
    "        checkpoint_name = f\"{time.time()}_epoch_{epoch}\"\n",
    "        save_model(epoch, model, optimizer, training_loss, rewards, checkpoint_name)\n",
    "\n",
    "        # Check parameters for NaNs.\n",
    "        for name, param in model.named_parameters():\n",
    "            if torch.isnan(param).any():\n",
    "                print(f\"NaN detected in parameters for {name} at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "    return training_loss, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = PolicyNetwork(state_size, state_size, device=DEVICE).to(DEVICE)\n",
    "model = DiffusionModel(policy_net, device=DEVICE)\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, rewards = train_rl(model, optimizer, reward_function, state_size,nb_epochs=300,  device=DEVICE, max_norm=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(121)\n",
    "plt.title('Loss')\n",
    "plt.plot(loss)\n",
    "plt.subplot(122)\n",
    "plt.title('Reward')\n",
    "plt.plot([r.item() for r in rewards[:]])\n",
    "# plt.plot([r for r in rewards[:]])\n",
    "plt.savefig('../results/Imgs/sub-'+model_name+'/'+model_name+'_train.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = [r.item() for r in rewards[:]]\n",
    "\n",
    "def exponential_moving_average(data, alpha=0.1):\n",
    "    ema = []\n",
    "    for i, value in enumerate(data):\n",
    "        if i == 0:\n",
    "            ema.append(value)\n",
    "        else:\n",
    "            ema.append(alpha * value + (1 - alpha) * ema[-1])\n",
    "    return ema\n",
    "\n",
    "# Smooth the rewards with EMA\n",
    "alpha = 0.1  # Smoothing factor\n",
    "smoothed_rewards_ema = exponential_moving_average(rewards, alpha)\n",
    "\n",
    "# Plot the original and smoothed rewards\n",
    "plt.plot(rewards, label='Original Rewards', alpha=0.5)\n",
    "plt.plot(smoothed_rewards_ema, label='Smoothed Rewards (EMA)', linewidth=2)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward Curve with EMA Smoothing')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p12",
   "language": "python",
   "name": "p12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
