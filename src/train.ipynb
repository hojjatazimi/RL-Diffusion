{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import torch.utils.data\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from time import time\n",
    "import pandas as pd\n",
    "from classes.policy_network import PolicyNetwork\n",
    "from classes.diffusion_model import DiffusionModel\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'mps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_batch(size):\n",
    "    x, _ = make_swiss_roll(size)\n",
    "    x = x[:, [2, 0]] / 10.0 * np.array([1, -1])\n",
    "    return x[:, 0].reshape((1, size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(features, weights, bias=None):\n",
    "    # Calculate the linear combination\n",
    "    logits = torch.matmul(features, weights)\n",
    "\n",
    "    # If bias is provided, add it to the logits\n",
    "    if bias is not None:\n",
    "        logits += bias\n",
    "\n",
    "    # Apply the sigmoid function to get the probabilities\n",
    "    probabilities = torch.sigmoid(logits)\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"01\"\n",
    "path_to_weights = (\n",
    "    \"/Users/hazimiasad/Documents/Work/megan/data/collection/Study1/sub-\"\n",
    "    + model_name\n",
    "    + \"/pattern/dc_weights.csv\"\n",
    ")\n",
    "weights = torch.from_numpy(pd.read_csv(path_to_weights, header=None).values.T).to(\n",
    "    DEVICE, dtype=torch.float32\n",
    ")\n",
    "state_size = len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(epoch, model, optimizer, loss, rewards, name, batch_size=32, max_grad_norm=1):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'batch_size':batch_size,\n",
    "        'max_grad_norm':max_grad_norm,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "        'rewards': rewards\n",
    "    }, \"/Users/hazimiasad/Documents/Work/megan/code/playground/RL-Diffusion/results/models/gradient/sub-\"+model_name+\"/\"+name+\".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rl(model, optimizer, reward_function, state_size, nb_epochs=150_000, batch_size=32, device='cpu', max_norm=.5):\n",
    "    training_loss = []\n",
    "    rewards = []\n",
    "    for epoch in tqdm(range(nb_epochs), mininterval=60.0):\n",
    "        batch_loss = 0\n",
    "        batch_reward = 0\n",
    "        for b in range(batch_size):\n",
    "            x0 = torch.from_numpy(sample_batch(state_size)).float().to(device)\n",
    "            t = 40\n",
    "            mu_posterior, sigma_posterior, x = model.forward_process(x0, t)\n",
    "            log_probs = []\n",
    "            for t in range(41, 1, -1):\n",
    "                # mu, sigma, x = model.reverse(x, t)\n",
    "                action, log_prob, _, _ = model.select_action(x, t)\n",
    "                log_probs.append(log_prob)\n",
    "                x = x + action\n",
    "        \n",
    "\n",
    "            reward = reward_function(x, weights)\n",
    "            batch_reward += reward\n",
    "            reward = torch.tensor(reward, dtype=torch.float32).to(device)\n",
    "            if torch.isnan(reward).any():\n",
    "                print(f\"NaN detected in reward at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "            log_probs_stack_sum = torch.stack(log_probs).sum()\n",
    "            loss = -reward * log_probs_stack_sum\n",
    "            batch_loss += loss\n",
    "        loss = batch_loss/batch_size\n",
    "        reward = batch_reward/batch_size\n",
    "        if torch.isnan(loss).any():\n",
    "            print(f\"NaN detected in loss at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Check gradients\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None and torch.isnan(param.grad).any():\n",
    "                print(f\"NaN detected in gradients for {name} at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "        optimizer.step()\n",
    "        training_loss.append(loss.item())\n",
    "        rewards.append(reward)\n",
    "        name = str(time.time()) +'_epoch_'+str(epoch)\n",
    "        save_model(epoch, model, optimizer, training_loss, rewards, name)\n",
    "        # Check parameters\n",
    "        for name, param in model.named_parameters():\n",
    "            if torch.isnan(param).any():\n",
    "                print(f\"NaN detected in parameters for {name} at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "    return training_loss, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reward(model, optimizer, reward_function, state_size, nb_epochs=150_000, batch_size=32, device='cpu', max_norm=0.5):\n",
    "    \"\"\"\n",
    "    Trains the diffusion model to maximize a reward signal computed on the final output.\n",
    "    This version does not use RL (e.g. REINFORCE) but instead backpropagates directly through\n",
    "    a deterministic reverse diffusion process.\n",
    "\n",
    "    Args:\n",
    "        model: the diffusion model (which must implement forward_process() and reverse())\n",
    "        optimizer: an optimizer for the model parameters\n",
    "        reward_function: a callable that takes (x, weights) and returns a reward (assumed differentiable)\n",
    "        state_size: size/shape information for sample_batch\n",
    "        nb_epochs: number of training epochs\n",
    "        batch_size: batch size per epoch\n",
    "        device: device to use ('cpu' or 'cuda')\n",
    "        max_norm: maximum gradient norm for clipping\n",
    "\n",
    "    Returns:\n",
    "        training_loss: list of loss values per epoch\n",
    "        rewards: list of average rewards per epoch\n",
    "    \"\"\"\n",
    "    training_loss = []\n",
    "    rewards = []\n",
    "\n",
    "    for epoch in tqdm(range(nb_epochs), mininterval=60.0):\n",
    "        batch_loss = 0.0\n",
    "        batch_reward = 0.0\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            # Sample an initial state x0\n",
    "            x0 = torch.from_numpy(sample_batch(state_size)).float().to(device)\n",
    "            t = 40\n",
    "\n",
    "            # Run the forward process (adding noise) to obtain a noisy sample x\n",
    "            mu_posterior, sigma_posterior, x = model.forward_process(x0, t)\n",
    "\n",
    "            # Instead of sampling actions via a policy (as in RL), run a deterministic reverse process\n",
    "            # For each diffusion step, use the model's reverse function to gradually denoise x.\n",
    "            for step in range(40, 0, -1):\n",
    "                # Here we assume model.reverse(x, t) returns the denoised x at step t.\n",
    "                # (If your model requires stochasticity you may need to adjust this accordingly.)\n",
    "                x = model.reverse(x, step)\n",
    "                # print(type(mean))\n",
    "                # print(type(std))\n",
    "                # print(type(nex))\n",
    "                # x = mean\n",
    "\n",
    "            # Compute the reward signal from the final sample x.\n",
    "            # (Make sure that the variable `weights` is defined or passed as an argument as needed.)\n",
    "            reward = reward_function(x, weights)\n",
    "            # Convert reward to a tensor on the correct device.\n",
    "            # reward_tensor = torch.tensor(reward_val, dtype=torch.float32, device=device)\n",
    "            # if torch.isnan(reward_tensor).any():\n",
    "            #     print(f\"NaN detected in reward at epoch {epoch}\")\n",
    "            #     continue\n",
    "\n",
    "            # We want to maximize the reward, so we minimize its negative.\n",
    "            loss = -reward\n",
    "            batch_loss += loss\n",
    "            batch_reward += reward.item()\n",
    "\n",
    "        # Average the losses and rewards over the batch.\n",
    "        loss = batch_loss / batch_size\n",
    "        avg_reward = batch_reward / batch_size\n",
    "        # print('LOSS:', loss)\n",
    "        if torch.isnan(loss).any():\n",
    "            print(f\"NaN detected in loss at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Check gradients for NaNs.\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None and torch.isnan(param.grad).any():\n",
    "                print(f\"NaN detected in gradients for {name} at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        training_loss.append(loss.item())\n",
    "        rewards.append(avg_reward)\n",
    "        checkpoint_name = f\"{time.time()}_epoch_{epoch}\"\n",
    "        save_model(epoch, model, optimizer, training_loss, rewards, checkpoint_name)\n",
    "\n",
    "        # Check parameters for NaNs.\n",
    "        for name, param in model.named_parameters():\n",
    "            if torch.isnan(param).any():\n",
    "                print(f\"NaN detected in parameters for {name} at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "    return training_loss, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = PolicyNetwork(state_size, state_size, device=DEVICE).to(DEVICE)\n",
    "model = DiffusionModel(policy_net, device=DEVICE)\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: tensor([[-0.5104]], device='mps:0', grad_fn=<DivBackward0>)\n",
      "LOSS: tensor([[-0.5126]], device='mps:0', grad_fn=<DivBackward0>)\n",
      "LOSS: tensor([[-0.5146]], device='mps:0', grad_fn=<DivBackward0>)\n",
      "LOSS: tensor([[-0.5167]], device='mps:0', grad_fn=<DivBackward0>)\n",
      "LOSS: tensor([[-0.5188]], device='mps:0', grad_fn=<DivBackward0>)\n",
      "LOSS: tensor([[-0.5209]], device='mps:0', grad_fn=<DivBackward0>)\n",
      "LOSS: tensor([[-0.5230]], device='mps:0', grad_fn=<DivBackward0>)\n",
      "LOSS: tensor([[-0.5251]], device='mps:0', grad_fn=<DivBackward0>)\n",
      "LOSS: tensor([[-0.5272]], device='mps:0', grad_fn=<DivBackward0>)\n",
      "LOSS: tensor([[-0.5294]], device='mps:0', grad_fn=<DivBackward0>)\n",
      "LOSS: tensor([[-0.5315]], device='mps:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 11/300 [00:30<13:14,  2.75s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loss, rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_reward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnb_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 41\u001b[0m, in \u001b[0;36mtrain_reward\u001b[0;34m(model, optimizer, reward_function, state_size, nb_epochs, batch_size, device, max_norm)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Instead of sampling actions via a policy (as in RL), run a deterministic reverse process\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# For each diffusion step, use the model's reverse function to gradually denoise x.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m40\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Here we assume model.reverse(x, t) returns the denoised x at step t.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# (If your model requires stochasticity you may need to adjust this accordingly.)\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# print(type(mean))\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# print(type(std))\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# print(type(nex))\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Compute the reward signal from the final sample x.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# (Make sure that the variable `weights` is defined or passed as an argument as needed.)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m reward \u001b[38;5;241m=\u001b[39m reward_function(x, weights)\n",
      "File \u001b[0;32m~/Documents/Work/megan/code/playground/RL-Diffusion/src/classes/diffusion_model.py:80\u001b[0m, in \u001b[0;36mDiffusionModel.reverse\u001b[0;34m(self, xt, t)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xt\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Predict mean and log-variance from the model\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m mean, std \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m x_prev \u001b[38;5;241m=\u001b[39m mean\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Sample new data point\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# noise = torch.randn_like(xt)\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \n\u001b[1;32m     85\u001b[0m \n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Sanity check for NaN values\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Work/megan/code/envs/p12/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Work/megan/code/envs/p12/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Work/megan/code/playground/RL-Diffusion/src/classes/policy_network.py:16\u001b[0m, in \u001b[0;36mPolicyNetwork.forward\u001b[0;34m(self, state, t)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, t):\n\u001b[0;32m---> 16\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     t \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     18\u001b[0m     state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([state, t], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss, rewards = train_reward(model, optimizer, reward_function, state_size,nb_epochs=300,  device=DEVICE, max_norm=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss, rewards = train_rl(model, optimizer, reward_function, state_size,nb_epochs=300,  device=DEVICE, max_norm=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(121)\n",
    "plt.title('Loss')\n",
    "plt.plot(loss)\n",
    "plt.subplot(122)\n",
    "plt.title('Reward')\n",
    "plt.plot([r.item() for r in rewards[:]])\n",
    "# plt.plot([r for r in rewards[:]])\n",
    "plt.savefig('../results/Imgs/sub-'+model_name+'/'+model_name+'_train.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = [r.item() for r in rewards[:]]\n",
    "\n",
    "def exponential_moving_average(data, alpha=0.1):\n",
    "    ema = []\n",
    "    for i, value in enumerate(data):\n",
    "        if i == 0:\n",
    "            ema.append(value)\n",
    "        else:\n",
    "            ema.append(alpha * value + (1 - alpha) * ema[-1])\n",
    "    return ema\n",
    "\n",
    "# Smooth the rewards with EMA\n",
    "alpha = 0.1  # Smoothing factor\n",
    "smoothed_rewards_ema = exponential_moving_average(rewards, alpha)\n",
    "\n",
    "# Plot the original and smoothed rewards\n",
    "plt.plot(rewards, label='Original Rewards', alpha=0.5)\n",
    "plt.plot(smoothed_rewards_ema, label='Smoothed Rewards (EMA)', linewidth=2)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward Curve with EMA Smoothing')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p12",
   "language": "python",
   "name": "p12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
